---
---

@article{anonymous2023metalearning,
  abbr={NeurIPS},
  title={Meta-Learning with Neural Bandit Scheduler},
  author={Qi,Yunzhe and Ban,Yikun and Wei,Tianxin and Zou,Jiaru and Yao,Huaxiu and He,Jingrui},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  html ={https://openreview.net/forum?id=Z2L7F0nekb},
  selected = {true},
  pdf = {metalearning.pdf},
  abstract = {Meta-learning has been proven an effective learning paradigm for training machine learning models with good generalization ability. Apart from the common practice of uniformly sampling the meta-training tasks, existing methods working on task scheduling strategies are all based on pre-defined sampling protocols or the assumed task-model correlations, and greedily make scheduling decisions, which can lead to sub-optimal performance bottlenecks of the meta-model. In this paper, we propose a novel task scheduling framework under Contextual Bandits settings, named BASS, which directly optimizes the task scheduling strategy based on the status of the meta-model. By balancing the exploitation and exploration, our framework can deal with the data scarcity problem at the early stage of meta-training while planning for the upcoming meta-training iterations with the adaptive exploration strategy. Theoretical analysis and extensive experiments are presented to show the effectiveness of our proposed framework.}
}

@article{anonymous2023TAP4LLM,
  abbr={ArXiv},
  title={TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning},
  author={Sui,Yuan and Zou,Jiaru and Zhou,Mengyu and He,Xinyi and Du,Lun and Han,Shi, and Zhang,Dongmei},
  year={2023},
  selected = {true},
  arxiv = {2312.09039},
  pdf = {Table_Provider.pdf},
  abstract = {Submitted to VLDB2024;Table reasoning has shown remarkable progress in a wide range of table-based tasks. This challenging task requires reasoning over both free-form natural language (NL) questions and semi-structured tabular data. However, previous table reasoning solutions suffer from significant performance degradation on “huge” tables. In addition, most existing methods struggle to reason over complex questions since lacking of essential information or they are scattered in different places. To alleviate the above challenges, we exploit a table provider on versatile sampling, augmentation and packing methods to achieve effective table reasoning using large language models (LLMs), which 1) decomposes the raw table into sub-table with specific rows/columns based on the rules or semantic similarity; 2) augments the table information by extracting semantic and statistical metadata from the raw table, and retrieving relevant knowledge from trustworthy knowledge sources (e.g., Wolfram Alpha, Wikipedia). 3) packs the table information with the augmented knowledge into a sequence for LLMs reasoning while balancing the token allocation trade-off. Experiment results illustrate that TAP4LLM not only demonstrates commendable performance across various tabular reasoning tasks but also serves as a systematic frame-work. It allows for different components as plug-ins, enhancing LLMs’ understanding of structured data in diverse tabular tasks}
}
@article{anonymous2023PPB,
  abbr={ArXiv},
  title={Personalized PageRank Bandits and Applications},
  author={Ban*, Yikun and Zou*, Jiaru and  Li, Zihao and Fu, Dongqi and Kang,Jian and He,Jingrui},
  year={2023},
  selected = {true},
  abstract = {Submitted to ACMSIGKDD 2024}
  }

@poster{2022NCSA,
  abbr={GCOE Research Fair},
  title={CNN Model Pruning and Quantization for FPGA},
  author={Hu*,Chuxuan and Mai*,Hanlin and Zou*,Jiaru and Rejive,Joseph and Eustis,William and Kindratenko, Volodymyr},
  year={2022},
  selected = {false},
  poster = {poster_quantization.pdf},
  abstract = {In the pursuit of deploying deep learning models on resource-constrained devices, Convolutional Neural Network (CNN) optimization has emerged as a critical area of research. Our study presented a novel approach on the simultaneous application of pruning and quantization techniques to streamline VGG16 for efficient deployment on Field-Programmable Gate Arrays (FPGAs). Through pruning and post training quantization, we were able to achieve the integer-level model performance and reduce the model size by more than 4x. We also evaluated our approach on several image classification tasks. The result showed that the quantized model performed significantly on running time efficiency while maintaining the accuracy of the original model.}
}

@poster{2023MLP,
  abbr={GCOE Research Fair},
  title={Mathematical Token Classification},
  author={Zou*,Jiaru and Wang*, Qing and Kani, Nickvash},
  year={2023},
  selected = {false},
  abstract = {Mathematical token classification is a fundamental task in mathematical information retrieval. In this work, we aim to formulate and address the following classification problem: While knowing what textual definitions best describe a math token, can langugae models learn basic information about the token:1.If it's a variable, constant, or operator. 2.If it's a scalar or matrix 3.If it's a known global-, commonly-used entity or if it is locally-defined. We buit a new dataset, namely MTDE, containing extracted math tokens with corresponding context and attributes based on arXiv corpus and evaluate its effectiveness from several accuracy performance of LLMs and PLMs.}
}
