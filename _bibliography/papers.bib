---
---

@article{anonymous2023metalearning,
  abbr={NeurIPS},
  title={Meta-Learning with Neural Bandit Scheduler},
  author={Qi,Yunzhe and Ban,Yikun and Wei,Tianxin and Zou,Jiaru and Yao,Huaxiu and He,Jingrui},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  html ={https://openreview.net/forum?id=Z2L7F0nekb},
  selected = {true},
  pdf = {metalearning.pdf},
  abstract = {Meta-learning has been proven an effective learning paradigm for training machine learning models with good generalization ability. Apart from the common practice of uniformly sampling the meta-training tasks, existing methods working on task scheduling strategies are all based on pre-defined sampling protocols or the assumed task-model correlations, and greedily make scheduling decisions, which can lead to sub-optimal performance bottlenecks of the meta-model. In this paper, we propose a novel task scheduling framework under Contextual Bandits settings, named BASS, which directly optimizes the task scheduling strategy based on the status of the meta-model. By balancing the exploitation and exploration, our framework can deal with the data scarcity problem at the early stage of meta-training while planning for the upcoming meta-training iterations with the adaptive exploration strategy. Theoretical analysis and extensive experiments are presented to show the effectiveness of our proposed framework.}
}

@article{anonymous2023TAP4LLM,
  abbr={ArXiv},
  title={TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning},
  author={Sui,Yuan and Zou,Jiaru and Zhou,Mengyu and He,Xinyi and Du,Lun and Han,Shi, and Zhang,Dongmei},
  year={2023},
  selected = {true},
  arxiv = {2312.09039},
  pdf = {Table_Provider.pdf},
  abstract = {Submitted to VLDB2024;Table reasoning has shown remarkable progress in a wide range of table-based tasks. This challenging task requires reasoning over both free-form natural language (NL) questions and semi-structured tabular data. However, previous table reasoning solutions suffer from significant performance degradation on “huge” tables. In addition, most existing methods struggle to reason over complex questions since lacking of essential information or they are scattered in different places. To alleviate the above challenges, we exploit a table provider on versatile sampling, augmentation and packing methods to achieve effective table reasoning using large language models (LLMs), which 1) decomposes the raw table into sub-table with specific rows/columns based on the rules or semantic similarity; 2) augments the table information by extracting semantic and statistical metadata from the raw table, and retrieving relevant knowledge from trustworthy knowledge sources (e.g., Wolfram Alpha, Wikipedia). 3) packs the table information with the augmented knowledge into a sequence for LLMs reasoning while balancing the token allocation trade-off. Experiment results illustrate that TAP4LLM not only demonstrates commendable performance across various tabular reasoning tasks but also serves as a systematic frame-work. It allows for different components as plug-ins, enhancing LLMs’ understanding of structured data in diverse tabular tasks}
}

@article{anonymous2024COLINE,
  abbr={ArXiv},
  title={CONLINE: Complex Code Generation and Refinement with Online Searching and Correctness Testing},
  author={He, Xinyi and Zou,Jiaru and Lin,Yun and Zhou,Mengyu and Han,Shi and Yuan,Zejian and Zhang,Dongmei},
  year={2024},
  selected = {true},
  abstract = {Large Language Models (LLMs) have revolutionized code generation ability by converting natural language descriptions into executable code. However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack supplementary contents. To address these challenges, we introduce the CONLINE framework, which enhances code generation by incorporating planned online searches for information retrieval and automated correctness testing for iterative refinement. CONLINE also serializes the complex inputs and outputs to improve comprehension and generate test case to ensure the framework's adaptability for real-world applications. CONLINE is validated through rigorous experiments on the DS-1000 and ClassEval datasets. It shows that CONLINE substantially improves the quality of complex code generation, highlighting its potential to enhance the practicality and reliability of LLMs in generating intricate code.}
}

@article{anonymous2023PPB,
  abbr={ArXiv},
  title={Personalized PageRank Bandits and Applications},
  author={Ban*, Yikun and Zou*, Jiaru and  Li, Zihao and Fu, Dongqi and Kang,Jian and He,Jingrui},
  year={2023},
  selected = {true},
  abstract = {Personalized Pagerank (PPR) has shown to be a powerful tool in many applications such as recommender systems and web pagerank. With the evolving World Wide Web, the variants of PPR have been proposed to adapt to the evolution pattern of graph structure, where each change of the graph is initiated by an interaction. Under the literature that most existing works focus on the evolution pattern of graph structure, the goal of this paper is to empower pagerank with more knowledge from sequential interactions or decision-making. In this paper, we proposed the first bandit-based Personalized Pagerank algorithm, enriching the classic PPR with elements such as context, reward, and uncertainty measurement to minimize the cumulative regret in sequential decision-making. We adapt the proposed algorithm to online edge prediction and node classification and show that our hybrid algorithm performs considerably better than the classic PPR algorithm and bandit-based algorithm. Additionally, we conduct the theoretical regret analysis of the new algorithm, establishing that its regret upper bound aligns with the state-of-the-art literature on neural bandits.}
}

@poster{2022NCSA,
  abbr={GCOE Research Fair},
  title={CNN Model Pruning and Quantization for FPGA},
  author={Hu*,Chuxuan and Mai*,Hanlin and Zou*,Jiaru and Rejive,Joseph and Eustis,William and Kindratenko, Volodymyr},
  year={2022},
  selected = {false},
  poster = {poster_quantization.pdf},
  abstract = {In the pursuit of deploying deep learning models on resource-constrained devices, Convolutional Neural Network (CNN) optimization has emerged as a critical area of research. Our study presented a novel approach on the simultaneous application of pruning and quantization techniques to streamline VGG16 for efficient deployment on Field-Programmable Gate Arrays (FPGAs). Through pruning and post training quantization, we were able to achieve the integer-level model performance and reduce the model size by more than 4x. We also evaluated our approach on several image classification tasks. The result showed that the quantized model performed significantly on running time efficiency while maintaining the accuracy of the original model.}
}

@poster{2023MLP,
  abbr={GCOE Research Fair},
  title={Mathematical Token Classification},
  author={Zou*,Jiaru and Wang*, Qing and Kani, Nickvash},
  year={2023},
  selected = {false},
  abstract = {Mathematical token classification is a fundamental task in mathematical information retrieval. In this work, we aim to formulate and address the following classification problem: While knowing what textual definitions best describe a math token, can langugae models learn basic information about the token:1.If it's a variable, constant, or operator. 2.If it's a scalar or matrix 3.If it's a known global-, commonly-used entity or if it is locally-defined. We buit a new dataset, namely MTDE, containing extracted math tokens with corresponding context and attributes based on arXiv corpus and evaluate its effectiveness from several accuracy performance of LLMs and PLMs.}
}
